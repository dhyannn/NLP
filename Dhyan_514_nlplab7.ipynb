{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9CpmqXbdwFoopX01Lg0PC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhyannn/NLP/blob/main/Dhyan_514_nlplab7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycr2tB_1D1Ty",
        "outputId": "e28f78d7-4c6d-492f-97cc-83917801964d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK7jRr79J-Yi",
        "outputId": "0a8a4319-8be0-49d2-8701-8c84e6924e75"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNR94sgECsmF",
        "outputId": "c18171b2-6176-42d7-af07-c55789fd7b56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatize tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in filtered_tokens]\n",
        "\n",
        "    # Return preprocessed text as a string\n",
        "    return ' '.join(lemmatized_tokens)\n"
      ],
      "metadata": {
        "id": "gUFt6V7EIDWp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(token):\n",
        "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wn.ADJ,\n",
        "                \"N\": wn.NOUN,\n",
        "                \"V\": wn.VERB,\n",
        "                \"R\": wn.ADV}\n",
        "    return tag_dict.get(tag, wn.NOUN)\n",
        "\n",
        "def calculate_cosine_similarity(vector1, vector2):\n",
        "    dot_product = np.dot(vector1, vector2)\n",
        "    norm1 = np.linalg.norm(vector1)\n",
        "    norm2 = np.linalg.norm(vector2)\n",
        "    return dot_product / (norm1 * norm2)\n",
        "\n",
        "def calculate_jaccard_similarity(set1, set2):\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union"
      ],
      "metadata": {
        "id": "iYu7In9aIDTw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document1 = \"Natural language processing is a field of artificial intelligence.\"\n",
        "document2 = \"Machine learning techniques are used in natural language processing.\"\n",
        "\n",
        "# Preprocess the documents\n",
        "preprocessed_document1 = preprocess_text(document1)\n",
        "preprocessed_document2 = preprocess_text(document2)"
      ],
      "metadata": {
        "id": "RhivGjdGIDQU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform([preprocessed_document1, preprocessed_document2])\n",
        "\n",
        "# Get TF-IDF vectors for each document\n",
        "tfidf_vectors = tfidf_matrix.toarray()\n",
        "tfidf_vector1 = tfidf_vectors[0]\n",
        "tfidf_vector2 = tfidf_vectors[1]"
      ],
      "metadata": {
        "id": "NYuh0uKbIDO-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cosine similarity\n",
        "cosine_similarity = calculate_cosine_similarity(tfidf_vector1, tfidf_vector2)\n",
        "print(\"Cosine Similarity:\", cosine_similarity)\n",
        "\n",
        "# Calculate Jaccard similarity\n",
        "set1 = set(preprocessed_document1.split())\n",
        "set2 = set(preprocessed_document2.split())\n",
        "jaccard_similarity = calculate_jaccard_similarity(set1, set2)\n",
        "print(\"Jaccard Similarity:\", jaccard_similarity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVLvPYdlIDLs",
        "outputId": "f98e77b7-f84a-4617-fb85-2f0be1edcdba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.3041257418754935\n",
            "Jaccard Similarity: 0.36363636363636365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the Sentiment Analysis using Bayesian Classification."
      ],
      "metadata": {
        "id": "hzTWNRcnExtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import random"
      ],
      "metadata": {
        "id": "NwmIZJ09FDrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqDdlnsrFDoF",
        "outputId": "3e3a2b6e-5da6-460c-9f86-13117955bfe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(document):\n",
        "    words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features[word] = (word in words)\n",
        "    return features"
      ],
      "metadata": {
        "id": "aFBnKmXrFDmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the movie reviews dataset\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]"
      ],
      "metadata": {
        "id": "rxk6XH_dFDis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the dataset\n",
        "for i, (words, category) in enumerate(documents[:5]):  # Display the first 5 documents\n",
        "    print(f\"Document {i+1}:\")\n",
        "    print(\"Category:\", category)\n",
        "    print(\"Words:\", ' '.join(words[:100]))  # Display the first 100 words of the document\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M6Qsn9mHMqH",
        "outputId": "84c6b169-8c89-46f8-8731-daa8ab538722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "Category: pos\n",
            "Words: you ' ve seen this moment before , recently : a particularly troubled character senses danger of the paranormal kind when the room temperature inexplicably plummets to below freezing . the difference is that when it happens to lili taylor ' s nell in the haunting , we don ' t care . the hero of the sixth sense , a young boy named cole , is a rich creation , and we wish nothing more than for the ghosts who haunt him to take a hike . the seasons have changed since an ex - patient shot jaded child\n",
            "\n",
            "Document 2:\n",
            "Category: neg\n",
            "Words: woof ! too bad that leap of faith was the title of a 1992 comedy starring steve martin and debra winger , because that ' s what ' s required to watch this incredulous howler starring bruce willis as -- of all things -- a psychologist . not since the reagan administration has there been an acting stretch of such magnitude ! alas , mickey rourke , we hardly knew ye . story opens with a campy kick -- willis is treating a patient who abruptly steps out of the window to take the best flying leap since charles durning\n",
            "\n",
            "Document 3:\n",
            "Category: neg\n",
            "Words: \" american pie \" alums jason biggs and mena suvari star in this summer ' s attempt to capitalize on the youth market looking for a comedy about young people they can relate to that combines generic hollywood cute couple - ness and zany comedy . it ' s about an a + student ( biggs as \" paul \" ) who gets a scholarship to some college in new york city who sticks out like a sore thumb and falls for dora ( suvari ) a ditzy heroin - chic goth chick who has no ambition or self -\n",
            "\n",
            "Document 4:\n",
            "Category: pos\n",
            "Words: titanic is , without a doubt , the best movie i ' ve seen all year . and believe me , i ' m not an easy critic to impress : the english patient was too long , men in black was over - hyped , and several of the \" big summer releases \" were dead in the water . thank god this film got delayed until december . it ' s my christmas present . titanic is in the tradition of the old hollywood epics of years - gone - by , and it is truly a stunning\n",
            "\n",
            "Document 5:\n",
            "Category: pos\n",
            "Words: _in brief : _ this film needs no introduction . if you haven ' t heard of it , then you must have been up in space on a sabbatical , and if you haven ' t seen it -- well , i suspect you ' re in a small minority ! first thing ' s first ? is it all it ' s hyped up to be ? well , yes ? and no . it ' s a good film , and there ' s a lot to like about it , but it ' s not without\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(documents)"
      ],
      "metadata": {
        "id": "UNF77wa1FDgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = [word.lower() for word in movie_reviews.words()]\n",
        "word_features = list(FreqDist(all_words).keys())[:2000]"
      ],
      "metadata": {
        "id": "TLkGhj3oFVj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featuresets = [(extract_features(doc), category) for (doc, category) in documents]"
      ],
      "metadata": {
        "id": "11CijIRRFVg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = featuresets[:1500], featuresets[1500:]"
      ],
      "metadata": {
        "id": "KVtJZwReFVeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Naive Bayes classifier\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)"
      ],
      "metadata": {
        "id": "z9lCEs4FFe_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the classifier\n",
        "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9OEdE9QFe8Z",
        "outputId": "5955aed7-279a-4707-9033-f51de94710c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of sentiment analysis\n",
        "text = \"titanic is , without a doubt , the best movie i ' ve seen all year\"\n",
        "words = word_tokenize(text.lower())\n",
        "features = extract_features(words)\n",
        "sentiment = classifier.classify(features)\n",
        "print(\"Sentiment:\", sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iySNTJGhFe6k",
        "outputId": "3dd4b9c9-7101-4ee2-d1d7-ee25887f48a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the Sentiment Analysis using RNN."
      ],
      "metadata": {
        "id": "9jj8iVMdKHSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN, LSTM"
      ],
      "metadata": {
        "id": "FQEnCZChKHEP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters for the model and data processing\n",
        "max_features = 10000\n",
        "maxlen = 500\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "wkCsTpvUKHBR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDB dataset\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GGlMnnvKG9W",
        "outputId": "57087b6f-1472-48da-d212-c977345d1df9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 2s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences to ensure uniform length\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x5pt9HeKG7h",
        "outputId": "d955c5bd-ae0a-4991-aff2-9a417d86e339"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 500)\n",
            "x_test shape: (25000, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the RNN model\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLC7_X6BKG37",
        "outputId": "1a94363b-484b-46f1-a7ed-bdf35c0f9640"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "_J90J-ofKG2A"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "print('Train...')\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=3, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yilUhQ7UKGyY",
        "outputId": "96e492fc-719d-47f9-971f-81ee07553dab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train...\n",
            "Epoch 1/3\n",
            "782/782 [==============================] - 116s 148ms/step - loss: 0.3399 - accuracy: 0.8578 - val_loss: 0.3543 - val_accuracy: 0.8612\n",
            "Epoch 2/3\n",
            "782/782 [==============================] - 111s 142ms/step - loss: 0.2634 - accuracy: 0.8986 - val_loss: 0.3477 - val_accuracy: 0.8613\n",
            "Epoch 3/3\n",
            "782/782 [==============================] - 109s 139ms/step - loss: 0.2107 - accuracy: 0.9193 - val_loss: 0.4381 - val_accuracy: 0.8038\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b598512d240>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYBK9sxuKGws",
        "outputId": "3f05c6fb-d9c5-46dd-8e29-92592ec52fa5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 21s 26ms/step - loss: 0.4381 - accuracy: 0.8038\n",
            "Test score: 0.4381325840950012\n",
            "Test accuracy: 0.8037999868392944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the Sentiment Analysis using LSTM."
      ],
      "metadata": {
        "id": "CroB4iEjR2Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDB dataset\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPzHHmhWKGtd",
        "outputId": "913fdb3c-bb10-44dc-87c0-2b8d9e4627d5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 500)\n",
            "x_test shape: (25000, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LSTM model\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM_52ZgKS6L4",
        "outputId": "f01608a4-1d70-4158-da72-7e3b5f0746f5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "rLo7nFqiKGrn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train...')\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=3, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc_iy8e6SZz-",
        "outputId": "0879dbb5-1fc9-4852-f952-f5ec5e4d70a8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train...\n",
            "Epoch 1/3\n",
            "782/782 [==============================] - 185s 236ms/step - loss: 0.2788 - accuracy: 0.8912 - val_loss: 0.3184 - val_accuracy: 0.8672\n",
            "Epoch 2/3\n",
            "782/782 [==============================] - 194s 248ms/step - loss: 0.2361 - accuracy: 0.9108 - val_loss: 0.3284 - val_accuracy: 0.8634\n",
            "Epoch 3/3\n",
            "782/782 [==============================] - 193s 247ms/step - loss: 0.2090 - accuracy: 0.9216 - val_loss: 0.2924 - val_accuracy: 0.8803\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b5982bf71c0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiFMLcFmSbZB",
        "outputId": "f7c5e6f1-a150-4657-82d6-01c3f169a01a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 36s 46ms/step - loss: 0.2924 - accuracy: 0.8803\n",
            "Test score: 0.2924209237098694\n",
            "Test accuracy: 0.8803200125694275\n"
          ]
        }
      ]
    }
  ]
}